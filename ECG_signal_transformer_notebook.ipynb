{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINALECG (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "d2PP5D-LIk2x"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SqlMt6XOJbt",
        "outputId": "694606f2-6190-4b75-bbfe-9bb4adf1895f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "id": "xyr4v7k-OJK2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PlBB0eYHCQ_",
        "outputId": "602e7ac6-3616-4c2c-c80a-3a449114e0db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SR_data = np.load('/content/drive/MyDrive/SR_100_samp.npy')"
      ],
      "metadata": {
        "id": "MGyGXzOuGfcv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AFIB_data=np.load('/content/drive/MyDrive/AFIB_100_samp.npy')"
      ],
      "metadata": {
        "id": "ySUVulxhHFkz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SR_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9Ng0P9NNt-s",
        "outputId": "fe057773-942c-4645-fb75-524bcb8ea9cd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 217, 334, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=SR_data[0].shape"
      ],
      "metadata": {
        "id": "sAFgUa6KNyFq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8Na2zKhN2sA",
        "outputId": "988170eb-5f49-433a-fec7-7a2faa04f694"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(217, 334, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_SR=np.zeros((1000,1))\n",
        "class_AFIB=np.ones((1000,1))\n",
        "Y_data=np.append(class_SR,class_AFIB)\n",
        "Y_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSzgQwA0HQtl",
        "outputId": "59ba56d6-3d3a-4645-a250-951ff3b1974b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=np.concatenate((SR_data,AFIB_data))"
      ],
      "metadata": {
        "id": "s0nQxPsMHjQb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc4UNT7wIJqy",
        "outputId": "53925d19-1b39-4d4a-c409-61d0f0094598"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 217, 334, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(data,Y_data,test_size=0.2,shuffle=True)\n"
      ],
      "metadata": {
        "id": "a4CBKjPtI428"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train=Y_train.reshape(1600,1)\n",
        "Y_test=Y_test.reshape(400,1)"
      ],
      "metadata": {
        "id": "I5WfU06ILv5l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOVOj2D3JGpQ",
        "outputId": "d2f13f98-b605-4432-dfae-ec0f8392385a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 2\n"
      ],
      "metadata": {
        "id": "hAyrS5qZNW3r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 100  # We'll resize input images to this size\n",
        "patch_size = 10  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
      ],
      "metadata": {
        "id": "M6XSQ8jfK_pX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "# Compute the mean and the variance of the training data for normalization.\n",
        "data_augmentation.layers[0].adapt(X_train)"
      ],
      "metadata": {
        "id": "SZx9kJ6aLXoe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "metadata": {
        "id": "utAzPxt3MU3w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "MKG_RolHNIMt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "2EPYWYAMMT4w"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(input_patches):\n",
        "        layer1 = layers.LayerNormalization(epsilon=1e-6)(input_patches) #normalization\n",
        "        attention_output = layers.MultiHeadAttention(  \n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(layer1, layer1)   #multiheaded attention\n",
        "        layer2 = layers.Add()([attention_output, input_patches]) #adding attention output with encoded patches\n",
        "        layer3 = layers.LayerNormalization(epsilon=1e-6)(layer2)  #normalization\n",
        "        layer3 = mlp(layer3, hidden_units=transformer_units, dropout_rate=0.1) #Multilayer perceptron\n",
        "        input_patches = layers.Add()([layer3, layer2])    #adding MLP output with normalized encoded patches output\n",
        "        return input_patches\n"
      ],
      "metadata": {
        "id": "PWRpDZIuiJvA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ECG_image_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs) #data augmentation\n",
        "    patches = Patches(patch_size)(augmented) #splitting image into patches\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "    for _ in range(transformer_layers):\n",
        "      encoded_patches=transformer_encoder(encoded_patches)   \n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)  #aggregating data from all transformer blocks and layers\n",
        "    representation = layers.Flatten()(representation)     #flatten the data for MLP\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    logits = layers.Dense(num_classes)(features)  #final binary classification \n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "51cKMKM3NKxS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(\n",
        "        x=X_train,\n",
        "        y=Y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(X_test, Y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "ECG_image_classifier = create_ECG_image_classifier()\n",
        "history = run_experiment(ECG_image_classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQM3DtMENOZb",
        "outputId": "5ce2ab22-c41a-48b7-a243-0748c8e7d816"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 217, 334, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " data_augmentation (Sequential)  (None, 100, 100, 3)  7          ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " patches_6 (Patches)            (None, None, 300)    0           ['data_augmentation[4][0]']      \n",
            "                                                                                                  \n",
            " patch_encoder_4 (PatchEncoder)  (None, 100, 64)     25664       ['patches_6[0][0]']              \n",
            "                                                                                                  \n",
            " layer_normalization_68 (LayerN  (None, 100, 64)     128         ['patch_encoder_4[0][0]']        \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_32 (Multi  (None, 100, 64)     66368       ['layer_normalization_68[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_68[0][0]'] \n",
            "                                                                                                  \n",
            " add_64 (Add)                   (None, 100, 64)      0           ['multi_head_attention_32[0][0]',\n",
            "                                                                  'patch_encoder_4[0][0]']        \n",
            "                                                                                                  \n",
            " layer_normalization_69 (LayerN  (None, 100, 64)     128         ['add_64[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_81 (Dense)               (None, 100, 128)     8320        ['layer_normalization_69[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 100, 128)     0           ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (None, 100, 64)      8256        ['dropout_76[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)           (None, 100, 64)      0           ['dense_82[0][0]']               \n",
            "                                                                                                  \n",
            " add_65 (Add)                   (None, 100, 64)      0           ['dropout_77[0][0]',             \n",
            "                                                                  'add_64[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_70 (LayerN  (None, 100, 64)     128         ['add_65[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_33 (Multi  (None, 100, 64)     66368       ['layer_normalization_70[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_70[0][0]'] \n",
            "                                                                                                  \n",
            " add_66 (Add)                   (None, 100, 64)      0           ['multi_head_attention_33[0][0]',\n",
            "                                                                  'add_65[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_71 (LayerN  (None, 100, 64)     128         ['add_66[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 100, 128)     8320        ['layer_normalization_71[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_78 (Dropout)           (None, 100, 128)     0           ['dense_83[0][0]']               \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 100, 64)      8256        ['dropout_78[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_79 (Dropout)           (None, 100, 64)      0           ['dense_84[0][0]']               \n",
            "                                                                                                  \n",
            " add_67 (Add)                   (None, 100, 64)      0           ['dropout_79[0][0]',             \n",
            "                                                                  'add_66[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_72 (LayerN  (None, 100, 64)     128         ['add_67[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_34 (Multi  (None, 100, 64)     66368       ['layer_normalization_72[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_72[0][0]'] \n",
            "                                                                                                  \n",
            " add_68 (Add)                   (None, 100, 64)      0           ['multi_head_attention_34[0][0]',\n",
            "                                                                  'add_67[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_73 (LayerN  (None, 100, 64)     128         ['add_68[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 100, 128)     8320        ['layer_normalization_73[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_80 (Dropout)           (None, 100, 128)     0           ['dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 100, 64)      8256        ['dropout_80[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_81 (Dropout)           (None, 100, 64)      0           ['dense_86[0][0]']               \n",
            "                                                                                                  \n",
            " add_69 (Add)                   (None, 100, 64)      0           ['dropout_81[0][0]',             \n",
            "                                                                  'add_68[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_74 (LayerN  (None, 100, 64)     128         ['add_69[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_35 (Multi  (None, 100, 64)     66368       ['layer_normalization_74[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_74[0][0]'] \n",
            "                                                                                                  \n",
            " add_70 (Add)                   (None, 100, 64)      0           ['multi_head_attention_35[0][0]',\n",
            "                                                                  'add_69[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_75 (LayerN  (None, 100, 64)     128         ['add_70[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 100, 128)     8320        ['layer_normalization_75[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_82 (Dropout)           (None, 100, 128)     0           ['dense_87[0][0]']               \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 100, 64)      8256        ['dropout_82[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_83 (Dropout)           (None, 100, 64)      0           ['dense_88[0][0]']               \n",
            "                                                                                                  \n",
            " add_71 (Add)                   (None, 100, 64)      0           ['dropout_83[0][0]',             \n",
            "                                                                  'add_70[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_76 (LayerN  (None, 100, 64)     128         ['add_71[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_36 (Multi  (None, 100, 64)     66368       ['layer_normalization_76[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_76[0][0]'] \n",
            "                                                                                                  \n",
            " add_72 (Add)                   (None, 100, 64)      0           ['multi_head_attention_36[0][0]',\n",
            "                                                                  'add_71[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_77 (LayerN  (None, 100, 64)     128         ['add_72[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 100, 128)     8320        ['layer_normalization_77[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_84 (Dropout)           (None, 100, 128)     0           ['dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 100, 64)      8256        ['dropout_84[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_85 (Dropout)           (None, 100, 64)      0           ['dense_90[0][0]']               \n",
            "                                                                                                  \n",
            " add_73 (Add)                   (None, 100, 64)      0           ['dropout_85[0][0]',             \n",
            "                                                                  'add_72[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_78 (LayerN  (None, 100, 64)     128         ['add_73[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_37 (Multi  (None, 100, 64)     66368       ['layer_normalization_78[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_78[0][0]'] \n",
            "                                                                                                  \n",
            " add_74 (Add)                   (None, 100, 64)      0           ['multi_head_attention_37[0][0]',\n",
            "                                                                  'add_73[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_79 (LayerN  (None, 100, 64)     128         ['add_74[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 100, 128)     8320        ['layer_normalization_79[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_86 (Dropout)           (None, 100, 128)     0           ['dense_91[0][0]']               \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 100, 64)      8256        ['dropout_86[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_87 (Dropout)           (None, 100, 64)      0           ['dense_92[0][0]']               \n",
            "                                                                                                  \n",
            " add_75 (Add)                   (None, 100, 64)      0           ['dropout_87[0][0]',             \n",
            "                                                                  'add_74[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_80 (LayerN  (None, 100, 64)     128         ['add_75[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_38 (Multi  (None, 100, 64)     66368       ['layer_normalization_80[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_80[0][0]'] \n",
            "                                                                                                  \n",
            " add_76 (Add)                   (None, 100, 64)      0           ['multi_head_attention_38[0][0]',\n",
            "                                                                  'add_75[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_81 (LayerN  (None, 100, 64)     128         ['add_76[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 100, 128)     8320        ['layer_normalization_81[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_88 (Dropout)           (None, 100, 128)     0           ['dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 100, 64)      8256        ['dropout_88[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_89 (Dropout)           (None, 100, 64)      0           ['dense_94[0][0]']               \n",
            "                                                                                                  \n",
            " add_77 (Add)                   (None, 100, 64)      0           ['dropout_89[0][0]',             \n",
            "                                                                  'add_76[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_82 (LayerN  (None, 100, 64)     128         ['add_77[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 100, 64)     66368       ['layer_normalization_82[0][0]', \n",
            " HeadAttention)                                                   'layer_normalization_82[0][0]'] \n",
            "                                                                                                  \n",
            " add_78 (Add)                   (None, 100, 64)      0           ['multi_head_attention_39[0][0]',\n",
            "                                                                  'add_77[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_83 (LayerN  (None, 100, 64)     128         ['add_78[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 100, 128)     8320        ['layer_normalization_83[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_90 (Dropout)           (None, 100, 128)     0           ['dense_95[0][0]']               \n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 100, 64)      8256        ['dropout_90[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)           (None, 100, 64)      0           ['dense_96[0][0]']               \n",
            "                                                                                                  \n",
            " add_79 (Add)                   (None, 100, 64)      0           ['dropout_91[0][0]',             \n",
            "                                                                  'add_78[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_84 (LayerN  (None, 100, 64)     128         ['add_79[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 6400)         0           ['layer_normalization_84[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 6400)         0           ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (None, 2048)         13109248    ['dropout_92[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 2048)         0           ['dense_97[0][0]']               \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 1024)         2098176     ['dropout_93[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)           (None, 1024)         0           ['dense_98[0][0]']               \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 2)            2050        ['dropout_94[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15,900,873\n",
            "Trainable params: 15,900,866\n",
            "Non-trainable params: 7\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "12/12 [==============================] - 97s 5s/step - loss: 4.3665 - accuracy: 0.5056 - top-5-accuracy: 1.0000 - val_loss: 0.9849 - val_accuracy: 0.5500 - val_top-5-accuracy: 1.0000\n",
            "Epoch 2/30\n",
            "12/12 [==============================] - 54s 5s/step - loss: 0.8929 - accuracy: 0.5653 - top-5-accuracy: 1.0000 - val_loss: 0.5414 - val_accuracy: 0.7375 - val_top-5-accuracy: 1.0000\n",
            "Epoch 3/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.6663 - accuracy: 0.6639 - top-5-accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.7312 - val_top-5-accuracy: 1.0000\n",
            "Epoch 4/30\n",
            "12/12 [==============================] - 54s 5s/step - loss: 0.6460 - accuracy: 0.6812 - top-5-accuracy: 1.0000 - val_loss: 0.4687 - val_accuracy: 0.7937 - val_top-5-accuracy: 1.0000\n",
            "Epoch 5/30\n",
            "12/12 [==============================] - 55s 5s/step - loss: 0.5564 - accuracy: 0.7194 - top-5-accuracy: 1.0000 - val_loss: 0.4804 - val_accuracy: 0.8000 - val_top-5-accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.5539 - accuracy: 0.7208 - top-5-accuracy: 1.0000 - val_loss: 0.4371 - val_accuracy: 0.7937 - val_top-5-accuracy: 1.0000\n",
            "Epoch 7/30\n",
            "12/12 [==============================] - 54s 5s/step - loss: 0.5050 - accuracy: 0.7375 - top-5-accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 0.8313 - val_top-5-accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "12/12 [==============================] - 55s 5s/step - loss: 0.4631 - accuracy: 0.7743 - top-5-accuracy: 1.0000 - val_loss: 0.3365 - val_accuracy: 0.8562 - val_top-5-accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "12/12 [==============================] - 54s 5s/step - loss: 0.4147 - accuracy: 0.8139 - top-5-accuracy: 1.0000 - val_loss: 0.2540 - val_accuracy: 0.8813 - val_top-5-accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.4070 - accuracy: 0.8285 - top-5-accuracy: 1.0000 - val_loss: 0.2969 - val_accuracy: 0.8750 - val_top-5-accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.3816 - accuracy: 0.8278 - top-5-accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.8750 - val_top-5-accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "12/12 [==============================] - 54s 5s/step - loss: 0.3423 - accuracy: 0.8500 - top-5-accuracy: 1.0000 - val_loss: 0.2343 - val_accuracy: 0.8875 - val_top-5-accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2702 - accuracy: 0.8903 - top-5-accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.8813 - val_top-5-accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2789 - accuracy: 0.8910 - top-5-accuracy: 1.0000 - val_loss: 0.2946 - val_accuracy: 0.8562 - val_top-5-accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "12/12 [==============================] - 54s 5s/step - loss: 0.2832 - accuracy: 0.8903 - top-5-accuracy: 1.0000 - val_loss: 0.2006 - val_accuracy: 0.9312 - val_top-5-accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2745 - accuracy: 0.8889 - top-5-accuracy: 1.0000 - val_loss: 0.2038 - val_accuracy: 0.9000 - val_top-5-accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2828 - accuracy: 0.8951 - top-5-accuracy: 1.0000 - val_loss: 0.2721 - val_accuracy: 0.8750 - val_top-5-accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2576 - accuracy: 0.9021 - top-5-accuracy: 1.0000 - val_loss: 0.2177 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2433 - accuracy: 0.9049 - top-5-accuracy: 1.0000 - val_loss: 0.2381 - val_accuracy: 0.9125 - val_top-5-accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2623 - accuracy: 0.8958 - top-5-accuracy: 1.0000 - val_loss: 0.2038 - val_accuracy: 0.9062 - val_top-5-accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2368 - accuracy: 0.9083 - top-5-accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9062 - val_top-5-accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2397 - accuracy: 0.9000 - top-5-accuracy: 1.0000 - val_loss: 0.1885 - val_accuracy: 0.9375 - val_top-5-accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2157 - accuracy: 0.9153 - top-5-accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 0.8313 - val_top-5-accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2702 - accuracy: 0.8944 - top-5-accuracy: 1.0000 - val_loss: 0.2271 - val_accuracy: 0.8938 - val_top-5-accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2044 - accuracy: 0.9181 - top-5-accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9187 - val_top-5-accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.1797 - accuracy: 0.9306 - top-5-accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9125 - val_top-5-accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2184 - accuracy: 0.9153 - top-5-accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.8938 - val_top-5-accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.2051 - accuracy: 0.9222 - top-5-accuracy: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.9187 - val_top-5-accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "12/12 [==============================] - 53s 4s/step - loss: 0.2292 - accuracy: 0.9167 - top-5-accuracy: 1.0000 - val_loss: 0.1873 - val_accuracy: 0.9000 - val_top-5-accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "12/12 [==============================] - 54s 4s/step - loss: 0.1703 - accuracy: 0.9306 - top-5-accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9438 - val_top-5-accuracy: 1.0000\n",
            "13/13 [==============================] - 5s 371ms/step - loss: 0.1666 - accuracy: 0.9450 - top-5-accuracy: 1.0000\n",
            "Test accuracy: 94.5%\n",
            "Test top 5 accuracy: 100.0%\n"
          ]
        }
      ]
    }
  ]
}